# Hadoop and MapReduce Overview

Hadoop and MapReduce facilitate the parallel processing of extensive batch computations over large datasets, though they may incur high latency. Hereâ€™s a brief overview of the key components and concepts:

## Key Components

- **Batch Layer:** Handles large-scale data storage and batch processing. This layer produces batch views from raw data.
- **Speed Layer:** Focuses on real-time data processing, providing low latency at the expense of some accuracy.
- **Lambda Architecture:** Distributes data across both the batch and speed layers for comprehensive processing.

## Big Data Challenges

Typical Big Data tasks involve:
- Iterating over a vast number of records.
- Extracting useful information from each record.
- Aggregating intermediate results.
- Generating a final output.

## Hadoop File System (HDFS)

HDFS divides data into chunks, each usually 64-128 MB, and replicates these chunks three times across different servers. This chunk-based system supports fault tolerance and distributed storage.

## MapReduce Programming Model

- **Map Phase:** Processes each element in the dataset, emitting a set of (key, value) pairs.
- **Reduce Phase:** Aggregates the (key, value) pairs generated by the Map phase, producing a final set of (key, value) pairs.
- **Shuffle and Sort:** Automatically handled by Hadoop, this phase groups (key, value) pairs by key after the Map phase.

### Key Classes in MapReduce

- **Driver:** Manages job configuration and submission.
  - `Configuration conf = this.getConf();`
  - `Job job = Job.getInstance(conf);`
  - Configures input and output formats, sets mapper and reducer classes, and handles job execution.

- **Mapper:** Defines the map operation.
  - `protected void map(MapperInputKeyType key, MapperInputValueType value, Context context) throws IOException, InterruptedException`
  - Processes input records and outputs (key, value) pairs.

- **Reducer:** Defines the reduce operation.
  - `protected void reduce(ReducerInputKeyType key, Iterable<ReducerInputValueType> values, Context context) throws IOException, InterruptedException`
  - Aggregates (key, value) pairs and outputs the results.

### Example: Word Count

- **Mapper:**
  ```java
  class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
          String[] words = value.toString().split("\\s+");
          for (String word : words) {
              context.write(new Text(word.toLowerCase()), new IntWritable(1));
          }
      }
  }
  ```

- **Reducer:**
  ```java
  class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
      protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
          int count = 0;
          for (IntWritable value : values) {
              count += value.get();
          }
          context.write(key, new IntWritable(count));
      }
  }
  ```

## Additional Concepts

- **Combiner:** A mini-reducer that performs partial aggregation on mapper output to reduce network traffic. It must be commutative and associative.
  ```java
  class WordCountCombiner extends Reducer<Text, IntWritable, Text, IntWritable> {
      protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
          int count = 0;
          for (IntWritable value : values) {
              count += value.get();
          }
          context.write(key, new IntWritable(count));
      }
  }
  ```

- **Configuration and Counters:**
  - **Driver Configuration:**
    ```java
    conf.set("name", "value");
    ```
  - **Accessing Counters:**
    ```java
    context.getCounter("group name", "counter name").increment(value);
    Counter counter = job.getCounters().findCounter("group name", "counter name");
    ```

- **Map-only Jobs:** Bypass the reduce phase and store the results directly in HDFS. Set the number of reducers to 0.

- **In-Mapper Combiner:** Improves efficiency by combining data within the mapper before emitting results. 

## Data Patterns

- **Summarization Patterns:**
  - **Numerical Summarizations:** Aggregate numerical data by groups.
  - **Inverted Index:** Create an index to facilitate faster searches.
  - **Counting with Counters:** Track counts using counters for summarization.

- **Filtering Patterns:**
  - **Filtering:** Remove unwanted records.
  - **Top K:** Identify top K records based on a ranking function.
  - **Distinct:** Find unique values or records.

- **Data Organization Patterns:**
  - **Binning:** Categorize data into bins.
  - **Shuffling:** Randomize the data order.

- **Metapatterns:** Organize complex workflows, such as job chaining.

- **Join Patterns:**
  - **Reduce Side Join:** Join relations using keys and concatenate records.
  - **Map Side Join:** Use a cached small table to join with a large table.

- **Multiple Inputs/Outputs:**
  - **Multiple Inputs:** Read from multiple datasets with consistent outputs.
  - **Multiple Outputs:** Store results in various files within the same directory.

- **Distributed Cache:** Share files across the cluster for use by mappers and reducers.

## MapReduce and Relational Algebra

- **Selection:** Filter records based on a condition.
- **Projection:** Select specific attributes from records.
- **Union:** Combine records from two relations.
- **Intersection:** Find common records between two relations.
- **Difference:** Find records in one relation but not in another.
- **Join:** Implement join operations as described above.
- **Aggregations and GroupBy:** Aggregate data by groups using summarization patterns.